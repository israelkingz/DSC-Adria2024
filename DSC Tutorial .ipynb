{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5e3c945c",
   "metadata": {},
   "source": [
    "## Speaker Details\n",
    "\n",
    "**Name**: Israel Odeajo \n",
    "**Expertise**: Data Science and Natural Language Processing Specialist  \n",
    "**Bio**: Israel Odeajo has over a decade of experience in machine learning and NLP, with a focus on developing scalable AI solutions for real-world applications. He has contributed to numerous open-source projects and is a regular speaker at international conferences.\n",
    "\n",
    "### Image\n",
    "\n",
    "<img src=\"mine.jpg\" alt=\"Speaker: Israel Odeajo\" width=\"200\" style=\"display: block; margin-left: auto; margin-right: auto;\"/>\n",
    "\n",
    "### Contact Information\n",
    "\n",
    "- **Email**: isrealodeajo@gmail.com\n",
    "- **LinkedIn**: [https://www.linkedin.com/in/odeajo-israel/](#)  \n",
    "- **Twitter**: [@israelkingz1](#)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98f5c3b1",
   "metadata": {},
   "source": [
    "https://colab.research.google.com/drive/1fARnUHu9WGaLKtdgtrulghP5zCxoeRiq#scrollTo=7ef288ef"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fbbcbe1",
   "metadata": {},
   "source": [
    "# What is NLP?\n",
    "\n",
    "NLP stands for Natural Language Processing. It is the branch of Artificial Intelligence that gives machines the ability to understand and process human languages. Human languages can be in the form of text or audio format.\n",
    "\n",
    "## History of NLP\n",
    "\n",
    "Natural Language Processing started in 1950 when Alan Mathison Turing published an article titled \"Computing Machinery and Intelligence\". This foundational work is based on Artificial Intelligence and discusses the automatic interpretation and generation of natural language. As technology has evolved, different approaches have emerged to deal with NLP tasks.\n",
    "\n",
    "\n",
    "## Applications of NLP\n",
    "\n",
    "The applications of Natural Language Processing include:\n",
    "\n",
    "- Text and speech processing, like Voice assistants â€“ Alexa, Siri, etc.\n",
    "- Text classification, like Grammarly, Microsoft Word, and Google Docs\n",
    "- Information extraction, like Search engines like DuckDuckGo, Google\n",
    "- Chatbots and Question Answering systems, like website bots\n",
    "- Language Translation, like Google Translate\n",
    "- Text summarization\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70b6573c",
   "metadata": {},
   "source": [
    "## How to Use RegEx in Python?\n",
    "\n",
    "To use Regular Expressions (RegEx) in Python, you first need to import the `re` module.\n",
    "\n",
    "### Example:\n",
    "\n",
    "Below is a Python code snippet that demonstrates how to use regular expressions to search for the word \"portal\" in a given string. It then prints the start and end indices of the matched word within the string.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "122f3172",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start Index: 34\n",
      "End Index: 40\n"
     ]
    }
   ],
   "source": [
    "import re \n",
    "\n",
    "s = 'GeeksforGeeks: A computer science portal for geeks'\n",
    "\n",
    "match = re.search(r'portal', s) \n",
    "\n",
    "print('Start Index:', match.start()) \n",
    "print('End Index:', match.end()) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e76b347",
   "metadata": {},
   "source": [
    "## Metacharacters\n",
    "\n",
    "Metacharacters are characters with special meanings in regular expressions. They play a crucial role in crafting patterns for matching and manipulating strings. Below is a list of metacharacters used in the `re` module functions along with their descriptions:\n",
    "\n",
    "| MetaCharacter | Description |\n",
    "|---------------|-------------|\n",
    "| `\\`           | Used to drop the special meaning of the character following it |\n",
    "| `[]`          | Represent a character class |\n",
    "| `^`           | Matches the beginning of a string |\n",
    "| `$`           | Matches the end of a string |\n",
    "| `.`           | Matches any character, except for a newline |\n",
    "| `|`           | Means OR (matches with any of the characters separated by it) |\n",
    "| `?`           | Matches zero or one occurrence of the preceding element |\n",
    "| `*`           | Matches zero or more occurrences of the preceding element |\n",
    "| `+`           | Matches one or more occurrences of the preceding element |\n",
    "| `{}`          | Indicate the number of occurrences of a preceding regex to match |\n",
    "| `()`          | Enclose a group of Regex patterns |\n",
    "\n",
    "Understanding and utilizing these metacharacters are fundamental to effectively using regular expressions for pattern matching and text manipulation.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fdc3810",
   "metadata": {},
   "source": [
    "# Tokenization in NLP\n",
    "\n",
    "Tokenization in natural language processing (NLP) is a foundational technique that involves dividing a sentence or phrase into smaller units, known as tokens. These tokens can include words, dates, punctuation marks, or even fragments of words. This article covers the basics of tokenization, its types, and its use cases.\n",
    "\n",
    "## What is Tokenization in NLP?\n",
    "\n",
    "Natural Language Processing (NLP) is a subfield of computer science, artificial intelligence, information engineering, and human-computer interaction. It focuses on programming computers to process and analyze large amounts of natural language data. The complexity of reading and understanding languages makes NLP a challenging field. Tokenization is a crucial first step in the NLP pipeline that influences the entire workflow.\n",
    "\n",
    "Tokenization is the process of breaking down text into smaller units, or tokens. These tokens are typically words or sub-words in natural language processing. It is a critical step in many NLP tasks, including text processing, language modeling, and machine translation. Tokens serve as the building blocks for further processing and analysis, enabling the conversion of unstructured text into a structured form suitable for machine learning.\n",
    "\n",
    "## Types of Tokenization\n",
    "\n",
    "Tokenization can be classified into several types based on the text segmentation method:\n",
    "\n",
    "### Word Tokenization\n",
    "\n",
    "- **Description**: Divides the text into individual words.\n",
    "- **Example**:  \n",
    "  Input: \"Tokenization is an important NLP task.\"  \n",
    "  Output: [\"Tokenization\", \"is\", \"an\", \"important\", \"NLP\", \"task\", \".\"]\n",
    "\n",
    "### Sentence Tokenization\n",
    "\n",
    "- **Description**: Segments the text into sentences.\n",
    "- **Example**:  \n",
    "  Input: \"Tokenization is an important NLP task. It helps break down text into smaller units.\"  \n",
    "  Output: [\"Tokenization is an important NLP task.\", \"It helps break down text into smaller units.\"]\n",
    "\n",
    "### Subword Tokenization\n",
    "\n",
    "- **Description**: Breaks down words into smaller units.\n",
    "- **Example**:  \n",
    "  Input: \"tokenization\"  \n",
    "  Output: [\"token\", \"ization\"]\n",
    "\n",
    "### Character Tokenization\n",
    "\n",
    "- **Description**: Divides the text into individual characters.\n",
    "- **Example**:  \n",
    "  Input: \"Tokenization\"  \n",
    "  Output: [\"T\", \"o\", \"k\", \"e\", \"n\", \"i\", \"z\", \"a\", \"t\", \"i\", \"o\", \"n\"]\n",
    "\n",
    "## Need of Tokenization\n",
    "\n",
    "Tokenization plays a crucial role in NLP for several reasons:\n",
    "\n",
    "- **Effective Text Processing**: Simplifies raw text for easier processing and analysis.\n",
    "- **Feature Extraction**: Enables numerical representation of text data for machine learning models.\n",
    "- **Language Modeling**: Facilitates organized representations of language for text generation and language modeling.\n",
    "- **Information Retrieval**: Essential for efficient indexing and searching based on words or phrases.\n",
    "- **Text Analysis**: Supports NLP tasks like sentiment analysis and named entity recognition.\n",
    "- **Vocabulary Management**: Helps manage a corpus's vocabulary by generating a list of distinct tokens.\n",
    "- **Task-Specific Adaptation**: Can be customized to suit specific NLP tasks such as summarization and machine translation.\n",
    "- **Preprocessing Step**: Transforms raw text into a format suitable for further statistical and computational analysis.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fc60e1c",
   "metadata": {},
   "source": [
    "# Tokenizing Text Using NLTK in Python\n",
    "\n",
    "To utilize the Natural Language Toolkit (NLTK) for text tokenization, it's necessary to first install NLTK on your system. NLTK is a vast library designed to assist with a wide range of Natural Language Processing (NLP) tasks, from tokenization to parsing and semantic reasoning.\n",
    "\n",
    "## Installation Process\n",
    "\n",
    "To install NLTK and set up your environment for NLP tasks, follow these steps:\n",
    "\n",
    "1. **Install NLTK**:\n",
    "   Open your terminal and execute the following command to install NLTK:\n",
    "\n",
    "    ```bash\n",
    "    sudo pip install nltk\n",
    "    ```\n",
    "\n",
    "2. **Enter Python Shell**:\n",
    "   After installation, enter the Python shell by typing `python` in your terminal.\n",
    "\n",
    "3. **Import NLTK and Download Resources**:\n",
    "   Inside the Python shell, import NLTK and download the necessary datasets, models, and resources by running:\n",
    "\n",
    "    ```python\n",
    "    import nltk\n",
    "    nltk.download('all')\n",
    "    ```\n",
    "\n",
    "    This step may take some time as NLTK includes a substantial amount of tokenizers, chunkers, algorithms, and corpora that need to be downloaded.\n",
    "\n",
    "## Understanding Key Terms\n",
    "\n",
    "Before diving into tokenization, let's clarify a few terms frequently encountered in NLP:\n",
    "\n",
    "- **Corpus**: A body of text, singular. \"Corpora\" is the plural form.\n",
    "- **Lexicon**: Essentially, a dictionary that includes words and their meanings.\n",
    "- **Token**: An individual piece or \"entity\" that results from breaking down text based on specific rules. For instance, a sentence can be tokenized into words, making each word a token. Similarly, paragraphs can be tokenized into sentences, making each sentence a token.\n",
    "\n",
    "Tokenization is crucial as it transforms text into a more manageable and analyzable format by splitting it into sentences, words, or other meaningful elements.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51c30007",
   "metadata": {},
   "outputs": [],
   "source": [
    "#importing the NLTK library for NLP task \n",
    "import nltk\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7af34601",
   "metadata": {},
   "source": [
    "The punkt package is a part of the Natural Language Toolkit (NLTK), which is a set of libraries and programs for symbolic and statistical natural language processing (NLP) for the Python programming language.\n",
    "\n",
    "punkt is specifically used for dividing a text into a list of sentences by using an unsupervised algorithm to build a model for abbreviation words, collocations, and words that start sentences. It is used for sentence tokenization, which means it can split text into sentences. This is particularly useful for applications that involve Natural Language Processing tasks like text summarization, sentiment analysis, and so on, where understanding the boundary of sentences is important."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39bb9849",
   "metadata": {},
   "outputs": [],
   "source": [
    "#tokenization\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce872f30",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"Natural language processing is an exciting area. Huge budget have been allocated for this.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4a6674b",
   "metadata": {},
   "outputs": [],
   "source": [
    "sent_tokenization = sent_tokenize(text)\n",
    "print(sent_tokenization)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1e209d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "word_token = word_tokenize(text)\n",
    "print(word_token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9c38628",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import the existing word and sentence tokenizing \n",
    "# libraries \n",
    "from nltk.tokenize import sent_tokenize, word_tokenize \n",
    "\n",
    "text = \"Natural language processing (NLP) is a field \" + \\ \n",
    "\t\"of computer science, artificial intelligence \" + \\ \n",
    "\t\"and computational linguistics concerned with \" + \\ \n",
    "\t\"the interactions between computers and human \" + \\ \n",
    "\t\"(natural) languages, and, in particular, \" + \\ \n",
    "\t\"concerned with programming computers to \" + \\ \n",
    "\t\"fruitfully process large natural language \" + \\ \n",
    "\t\"corpora. Challenges in natural language \" + \\ \n",
    "\t\"processing frequently involve natural \" + \\ \n",
    "\t\"language understanding, natural language\" + \\ \n",
    "\t\"generation frequently from formal, machine\" + \\ \n",
    "\t\"-readable logical forms), connecting language \" + \\ \n",
    "\t\"and machine perception, managing human-\" + \\ \n",
    "\t\"computer dialog systems, or some combination \" + \\ \n",
    "\t\"thereof.\"\n",
    "\n",
    "print(sent_tokenize(text)) \n",
    "print(word_tokenize(text))` \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e845b1d8",
   "metadata": {},
   "source": [
    "# Tokenizing Text Using TextBlob\n",
    "\n",
    "The TextBlob module is a Python library that provides a simple API for performing basic Natural Language Processing (NLP) tasks. It is built on top of the NLTK module, leveraging its rich set of features while offering an easier entry point for beginners.\n",
    "\n",
    "## Installing TextBlob\n",
    "\n",
    "To start using TextBlob for NLP tasks, including text tokenization, you need to install the library and its dependencies. Follow these steps to install TextBlob and download the necessary NLTK corpora:\n",
    "\n",
    "1. **Install TextBlob**:\n",
    "   Open your terminal and run the following command to install the TextBlob library:\n",
    "\n",
    "    ```bash\n",
    "    pip install -U textblob\n",
    "    ```\n",
    "\n",
    "2. **Download TextBlob Corpora**:\n",
    "   After installing TextBlob, you need to download the necessary data for NLP tasks. Execute the following command in your terminal:\n",
    "\n",
    "    ```bash\n",
    "    python -m textblob.download_corpora\n",
    "    ```\n",
    "\n",
    "   Note: The installation and download process may take some time due to the large volume of tokenizers, chunkers, algorithms, and corpora that need to be downloaded.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "520ca097",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from textblob lib. import TextBlob method \n",
    "from textblob import TextBlob \n",
    "\n",
    "text = (\"Natural language processing (NLP) is a field \" +\n",
    "\t\"of computer science, artificial intelligence \" +\n",
    "\t\"and computational linguistics concerned with \" +\n",
    "\t\"the interactions between computers and human \" +\n",
    "\t\"(natural) languages, and, in particular, \" +\n",
    "\t\"concerned with programming computers to \" +\n",
    "\t\"fruitfully process large natural language \" +\n",
    "\t\"corpora. Challenges in natural language \" +\n",
    "\t\"processing frequently involve natural \" +\n",
    "\t\"language understanding, natural language\" +\n",
    "\t\"generation frequently from formal, machine\" +\n",
    "\t\"-readable logical forms), connecting language \" +\n",
    "\t\"and machine perception, managing human-\" +\n",
    "\t\"computer dialog systems, or some combination \" +\n",
    "\t\"thereof.\") \n",
    "\t\n",
    "# create a TextBlob object \n",
    "blob_object = TextBlob(text) \n",
    "\n",
    "# tokenize paragraph into words. \n",
    "print(\" Word Tokenize :\\n\", blob_object.words) \n",
    "\n",
    "# tokenize paragraph into sentences. \n",
    "print(\"\\n Sentence Tokenize :\\n\", blob_object.sentences) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f39bf98b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9f7a051",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem.porter import PorterStemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9000919",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reduce words to their stems\n",
    "stemmed = [PorterStemmer().stem(w) for w in words]\n",
    "print(stemmed)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "515791c4",
   "metadata": {},
   "source": [
    "### What is Lemmatization? \n",
    "\n",
    "In contrast to stemming, lemmatization is a lot more powerful. It looks beyond word reduction and considers a languageâ€™s full vocabulary to apply a morphological analysis to words, aiming to remove inflectional endings only and to return the base or dictionary form of a word, which is known as the lemma."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ab33f06",
   "metadata": {},
   "source": [
    "# Wordnet Lemmatizer\n",
    "\n",
    "Wordnet is a comprehensive, publicly available lexical database supporting over 200 languages, known for providing semantic relationships between words. As one of the earliest and most widely used lemmatization techniques, it is a crucial tool in the field of Natural Language Processing (NLP).\n",
    "\n",
    "## Key Features of Wordnet\n",
    "\n",
    "- **Semantic Links**: Wordnet organizes words into semantic relations, such as synonyms, making it a valuable resource for understanding the meaning of words in context.\n",
    "- **Synsets**: Words are grouped into sets of synonyms, known as synsets, which represent a unique semantic concept. This grouping facilitates a deeper understanding of word semantics.\n",
    "\n",
    "## How to Use Wordnet Lemmatizer in Python\n",
    "\n",
    "Wordnet Lemmatizer is part of the NLTK (Natural Language Toolkit) library in Python, which offers a vast suite of tools for language processing.\n",
    "\n",
    "### Installing NLTK and Wordnet\n",
    "\n",
    "To use Wordnet Lemmatizer, you first need to install the NLTK package and then download the Wordnet data via NLTK. Here are the steps:\n",
    "\n",
    "1. **Install NLTK**:\n",
    "   Open your terminal or Anaconda prompt and install NLTK by running:\n",
    "\n",
    "    ```bash\n",
    "    pip install nltk\n",
    "    ```\n",
    "\n",
    "2. **Download Wordnet and Required NLTK Data**:\n",
    "   After installing NLTK, you need to download the Wordnet dataset and the 'averaged_perceptron_tagger' which is required for part-of-speech tagging. In your Python console, execute the following commands:\n",
    "\n",
    "    ```python\n",
    "    import nltk\n",
    "    nltk.download('wordnet')\n",
    "    nltk.download('averaged_perceptron_tagger')\n",
    "    ```\n",
    "\n",
    "By following these steps, you'll have Wordnet and the necessary components installed, enabling you to use the Wordnet Lemmatizer for processing and analyzing text data in Python.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "650957a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a243c395",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "# Reduce words to their root form\n",
    "lemmed = [WordNetLemmatizer().lemmatize(w) for w in words]\n",
    "print(lemmed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77d2529e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4953a95",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "example_sent = \"\"\"This is a sample sentence,\n",
    "\t\t\t\tshowing off the stop words filtration.\"\"\"\n",
    "\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "word_tokens = word_tokenize(example_sent)\n",
    "# converts the words in word_tokens to lower case and then checks whether\n",
    "#they are present in stop_words or not\n",
    "filtered_sentence = [w for w in word_tokens if not w.lower() in stop_words]\n",
    "#with no lower case conversion\n",
    "filtered_sentence = []\n",
    "\n",
    "for w in word_tokens:\n",
    "\tif w not in stop_words:\n",
    "\t\tfiltered_sentence.append(w)\n",
    "\n",
    "print(word_tokens)\n",
    "print(filtered_sentence)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "770d2a5b",
   "metadata": {},
   "source": [
    "Spacy is a library that comes under NLP (Natural Language Processing). It is an object-oriented Library that is used to deal with pre-processing of text, and sentences, and to extract information from the text using modules and functions.\n",
    "\n",
    "Tokenization is the process of splitting a text or a sentence into segments, which are called tokens. It is the first step of text preprocessing and is used as input for subsequent processes like text classification, lemmatization, etc."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "619fb82b",
   "metadata": {},
   "source": [
    "In natural language processing (NLP), stopwords are frequently filtered out to enhance text analysis and computational efficiency. Eliminating stopwords can improve the accuracy and relevance of NLP tasks by drawing attention to the more important words, or content words. The article aims to explore stopwords."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ec9be4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "\n",
    "# Load spaCy English model\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "# Sample text\n",
    "text = \"There is a pen on the table\"\n",
    "\n",
    "# Process the text using spaCy\n",
    "doc = nlp(text)\n",
    "\n",
    "# Remove stopwords\n",
    "filtered_words = [token.text for token in doc if not token.is_stop]\n",
    "\n",
    "# Join the filtered words to form a clean text\n",
    "clean_text = ' '.join(filtered_words)\n",
    "\n",
    "print(\"Original Text:\", text)\n",
    "print(\"Text after Stopword Removal:\", clean_text)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "180cef81",
   "metadata": {},
   "source": [
    "### Removing stop words with Genism"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f99feeab",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.parsing.preprocessing import remove_stopwords\n",
    "\n",
    "# Another sample text\n",
    "new_text = \"The majestic mountains provide a breathtaking view.\"\n",
    "\n",
    "# Remove stopwords using Gensim\n",
    "new_filtered_text = remove_stopwords(new_text)\n",
    "\n",
    "print(\"Original Text:\", new_text)\n",
    "print(\"Text after Stopword Removal:\", new_filtered_text)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3a44854",
   "metadata": {},
   "source": [
    "### Part of Speech Tagging using TextBlob\n",
    "TextBlob module is used for building programs for text analysis. One of the more powerful aspects of the TextBlob module is the Part of Speech tagging."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1180acd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "nltk.download('averaged_perceptron_tagger')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac45d5e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from textblob lib import TextBlob method\n",
    "from textblob import TextBlob\n",
    "\n",
    "text = (\"Sukanya, Rajib and Naba are my good friends. \" +\n",
    "\t\"Sukanya is getting married next year. \" +\n",
    "\t\"Marriage is a big step in oneâ€™s life.\" +\n",
    "\t\"It is both exciting and frightening. \" +\n",
    "\t\"But friendship is a sacred bond between people.\" +\n",
    "\t\"It is a special kind of love between us. \" +\n",
    "\t\"Many of you must have tried searching for a friend \"+\n",
    "\t\"but never found the right one.\")\n",
    "\n",
    "# create a textblob object\n",
    "blob_object = TextBlob(text)\n",
    "\n",
    "# Part-of-speech tags can be accessed\n",
    "# through the tags property of blob object.'\n",
    "\n",
    "# print word with pos tag.\n",
    "print(blob_object.tags)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1cf4f3a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5563e9bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# input string\n",
    "string = \"\t Python 3.0, released in 2008, was a major revision of the language that is not completely backward compatible and much Python 2 code does not run unmodified on Python 3. With Python 2's end-of-life, only Python 3.6.x[30] and later are supported, with older versions still supporting e.g. Windows 7 (and old installers not restricted to 64-bit Windows).\"\n",
    "print(string)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1037fb75",
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert to lower case\n",
    "lower_string = string.lower()\n",
    "print(lower_string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "312d4173",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "# remove numbers\n",
    "no_number_string = re.sub(r'\\d+','',lower_string)\n",
    "print(no_number_string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b2b04c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove all punctuation except words and space\n",
    "no_punc_string = re.sub(r'[^\\w\\s]','', no_number_string)\n",
    "print(no_punc_string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd0c32dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove white spaces\n",
    "no_wspace_string = no_punc_string.strip()\n",
    "print(no_wspace_string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ada47db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert string to list of words\n",
    "lst_string = [no_wspace_string][0].split()\n",
    "print(lst_string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76476a67",
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove stopwords\n",
    "no_stpwords_string=\"\"\n",
    "for i in lst_string:\n",
    "    if not i in stop_words:\n",
    "        no_stpwords_string += i+' '\n",
    "\n",
    "# removing last space\n",
    "no_stpwords_string = no_stpwords_string[:-1]\n",
    "\n",
    "# output\n",
    "print(no_stpwords_string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab562650",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Python3 code for preprocessing text\n",
    "import nltk\n",
    "import re\n",
    "import numpy as np\n",
    "\n",
    "# execute the text here as :\n",
    "text = \"Beans. I was trying to explain to somebody as we were flying in, thatâ€™s corn. Thatâ€™s beans. And they were very impressed at my agricultural knowledge. Please give it up for Amaury once again for that outstanding introduction. I have a bunch of good friends here today, including somebody who I served with, who is one of the finest senators in the country, and weâ€™re lucky to have him, your Senator, Dick Durbin is here. I also noticed, by the way, former Governor Edgar here, who I havenâ€™t seen in a long time, and somehow he has not aged and I have. And itâ€™s great to see you, Governor. I want to thank President Killeen and everybody at the U of I System for making it possible for me to be here today. And I am deeply honored at the Paul Douglas Award that is being given to me. He is somebody who set the path for so much outstanding public service here in Illinois. Now, I want to start by addressing the elephant in the room. I know people are still wondering why I didnâ€™t speak at the commencement.\"\n",
    "dataset = nltk.sent_tokenize(text)\n",
    "for i in range(len(dataset)):\n",
    "\tdataset[i] = dataset[i].lower()\n",
    "\tdataset[i] = re.sub(r'\\W', ' ', dataset[i])\n",
    "\tdataset[i] = re.sub(r'\\s+', ' ', dataset[i])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1f97499",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating the Bag of Words model\n",
    "word2count = {}\n",
    "for data in dataset:\n",
    "\twords = nltk.word_tokenize(data)\n",
    "\tfor word in words:\n",
    "\t\tif word not in word2count.keys():\n",
    "\t\t\tword2count[word] = 1\n",
    "\t\telse:\n",
    "\t\t\tword2count[word] += 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f4980b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import required module fior TF- IDF\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b7da262",
   "metadata": {},
   "outputs": [],
   "source": [
    "# assign documents\n",
    "d0 = 'Geeks for geeks'\n",
    "d1 = 'Geeks'\n",
    "d2 = 'r2j'\n",
    "\n",
    "# merge documents into a single corpus\n",
    "string = [d0, d1, d2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e63c68c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3aefd42",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create object\n",
    "tfidf = TfidfVectorizer()\n",
    "\n",
    "# get tf-df values\n",
    "result = tfidf.fit_transform(string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "baa03062",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get idf values\n",
    "print('\\nidf values:')\n",
    "for ele1, ele2 in zip(tfidf.get_feature_names_out(), tfidf.idf_):\n",
    "\tprint(ele1, ':', ele2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d409e215",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get indexing\n",
    "print('\\nWord indexes:')\n",
    "print(tfidf.vocabulary_)\n",
    "\n",
    "# display tf-idf values\n",
    "print('\\ntf-idf value:')\n",
    "print(result)\n",
    "\n",
    "# in matrix form\n",
    "print('\\ntf-idf values in matrix form:')\n",
    "print(result.toarray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e423dbf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk.download('punkt')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('vader_lexicon')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33b65d7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets = [\n",
    " \"dear @verizonsupport your service is straight ðŸ’© in dallas.. been with y'all over a decade and this is all time low for y'all. i'm talking no internet at all.\",\n",
    " \"@verizonsupport I sent you a dm\",\n",
    " \"thanks to michelle et al at @verizonsupport who helped push my no-show-phone problem along. Order canceled successfully, and I ordered this for pickup today at the Apple store in the mall.\"\n",
    " ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26fd3186",
   "metadata": {},
   "outputs": [],
   "source": [
    "for tweet in tweets:\n",
    "  blob = TextBlob(tweet)\n",
    "  print(blob.sentiment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dec4a218",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.sentiment import SentimentIntensityAnalyzer\n",
    "sia = SentimentIntensityAnalyzer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1baf24ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "for tweet in tweets:\n",
    "  scores = sia.polarity_scores(tweet)\n",
    "  print(scores)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
